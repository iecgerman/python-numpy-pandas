{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6dBgeih9w+r6trFlYb+hp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iecgerman/python-numpy-pandas/blob/master/clase12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQIRIx-tjDWE",
        "outputId": "3a940eba-e9aa-4272-ebfa-dceb1ed038a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando bloque 1\n",
            "Procesando bloque 2\n",
            "Procesando bloque 3\n",
            "Procesando bloque 4\n",
            "Procesando bloque 5\n",
            "Procesando bloque 6\n",
            "Procesando bloque 7\n",
            "Procesando bloque 8\n",
            "Procesando bloque 9\n",
            "Procesando bloque 10\n"
          ]
        }
      ],
      "source": [
        "# Optimización y escalabilidad en NumPy\n",
        "\n",
        "\"\"\"Dividir y Conquistar\n",
        "Una estrategia eficaz para manejar grandes volúmenes de datos es dividir los arrays en bloques más pequeños. Esto reduce la carga de memoria y facilita el procesamiento.\n",
        "\n",
        "Imagina que estás trabajando con el histórico de ventas de una tienda. Procesar toda la información de una vez puede ser impráctico, pero dividir los datos en bloques más pequeños puede hacer una gran diferencia.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Crear un array grande de 1 millon de elementos aleatorios\n",
        "large_array = np.random.rand(1000000)\n",
        "# print(large_array)\n",
        "\n",
        "# Dividir en bloques mas peque;os\n",
        "\n",
        "block_size = 100000\n",
        "\n",
        "for i in range(0, len(large_array), block_size):\n",
        "  block = large_array[i:i+block_size]\n",
        "  # Realizar operacion en el bloque\n",
        "  print(f\"Procesando bloque {i//block_size + 1}\")\n",
        "\n",
        "  \"\"\"Este código divide un array grande en bloques de 100,000 elementos para procesarlos por partes, evitando agotar la memoria del sistema. Esta técnica es fundamental para manejar grandes volúmenes de datos de manera eficiente.\n",
        "  \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Uso de Memoria y Optimización\n",
        "\n",
        "Optimizar el uso de memoria es crucial cuando se trabaja con grandes volúmenes de datos.\n",
        "\n",
        "Aquí te comparto dos estrategias efectivas:\n",
        "\n",
        "Tipos de Datos Eficientes\n",
        "\n",
        "Utilizar tipos de datos más eficientes puede reducir significativamente el uso de memoria. Por ejemplo, cambiar de float64 a float32 puede reducir el tamaño del array a la mitad, lo que es crucial cuando se trabaja con grandes volúmenes de datos\n",
        "\"\"\"\n",
        "\n",
        "# Array con tipo de dato float64\n",
        "array_float64 = np.array([1.0, 2.0, 3.0], dtype=np.float64)\n",
        "print(\"Uso de memoria float64:\", array_float64.nbytes)\n",
        "\n",
        "print(array_float64)\n",
        "\n",
        "\"\"\"En este ejemplo, se crean dos arrays: uno de tipo float64 y otro de tipo float32. Usar float32 en lugar de float64 puede reducir el uso de memoria a la mitad, lo que es esencial para eficientar el manejo de grandes datasets.\n",
        "\"\"\"\n",
        "\n",
        "# Array con tipo de dato float32\n",
        "array_float32 = np.array([1.0, 2.0, 3.0], dtype=np.float32)\n",
        "print(\"Uso de memoria float32:\", array_float32.nbytes)\n",
        "\n",
        "print(array_float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwXIOkWilrOy",
        "outputId": "810e4865-7490-4a34-81ac-6a21dec7758e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uso de memoria float64: 24\n",
            "[1. 2. 3.]\n",
            "Uso de memoria float32: 12\n",
            "[1. 2. 3.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Operaciones In-place\n",
        "\n",
        "Realizar operaciones in-place puede reducir el uso de memoria al evitar la creación de arrays temporales. Esto es especialmente útil cuando se necesita actualizar los valores de un array sin duplicarlo en la memoria.\n",
        "\"\"\"\n",
        "\n",
        "array = np.array([1, 2, 3, 4, 5])\n",
        "array += 1  # Operación in-place\n",
        "print(\"Array después de operación in-place:\", array)\n",
        "\n",
        "\n",
        "# Con +=1 realizamos una operación in-place sobre un array, incrementando cada elemento en 1. Las operaciones in-place modifican el array original sin crear uno nuevo, lo que ahorra memoria.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBxtWiYembOX",
        "outputId": "6100133a-018d-4078-d395-35a7113b3e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Array después de operación in-place: [2 3 4 5 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Operaciones Paralelas y Uso de Múltiples Núcleos\n",
        "\n",
        "# Además de optimizar el uso de memoria, es fundamental acelerar el procesamiento de grandes arrays. Para ello, es esencial utilizar operaciones paralelas y múltiples núcleos.\n",
        "\n",
        "# Uso de numexpr para Operaciones Paralelas\n",
        "\n",
        "# numexpr es una biblioteca que permite realizar operaciones numéricas de manera más eficiente utilizando múltiples núcleos. Esto puede acelerar significativamente el procesamiento de grandes arrays.\n",
        "\n",
        "import numexpr as ne\n",
        "import numpy as np\n",
        "\n",
        "# Crear dos arrays grandes\n",
        "\n",
        "a = np.random.rand(1000000)\n",
        "b = np.random.rand(1000000)\n",
        "\n",
        "# Usando numexpr para operación paralela\n",
        "\n",
        "result = ne.evaluate(\"a + b\")\n",
        "\n",
        "print(result)\n",
        "\n",
        "print(\"Operacion paralela completada con numexpr\")\n",
        "\n",
        "# Este código compara el tiempo de ejecución de una operación de suma en arrays grandes utilizando numexpr para realizar la operación en paralelo y sin numexpr.\n",
        "\n",
        "# Usar numexpr puede acelerar significativamente el procesamiento, lo que es crucial para manejar grandes volúmenes de datos de manera eficiente.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXnLbLRynZOQ",
        "outputId": "74431bbf-6be7-4857-a999-4065846f3dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.5853982  0.41182467 0.8594233  ... 1.79209837 0.4363096  1.37500662]\n",
            "Operacion paralela completada con numexpr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uso de joblib para Paralelización\n",
        "\n",
        "# joblib facilita la paralelización de tareas, permitiendo distribuir el trabajo entre múltiples núcleos del procesador. Esto es útil para tareas que pueden dividirse en partes independientes, como el procesamiento de grandes arrays.\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def process_block(block):\n",
        "  return np.sum(block)\n",
        "\n",
        "large_array = np.random.rand(1000000)\n",
        "block_size = 100000\n",
        "blocks = [large_array[i:i+block_size] for i in range (0, len(large_array), block_size)]\n",
        "\n",
        "results = Parallel(n_jobs=-1)(delayed(process_block)(block) for block in blocks)\n",
        "print(\"Resultados de la paralelización:\", results)\n",
        "\n",
        "# Utilizamos joblibpara paralelizar el procesamiento de un array grande dividido en bloques. Cada bloque se suma de forma paralela en múltiples núcleos del procesador, mejorando la eficiencia del procesamiento.\n",
        "\n",
        "# Manejar y optimizar arrays grandes es crucial en el análisis de datos para garantizar un rendimiento eficiente. Estrategias como dividir arrays en bloques, utilizar tipos de datos eficientes y realizar operaciones paralelas pueden mejorar significativamente la velocidad y eficiencia del procesamiento de datos. Estas técnicas son esenciales en diversos campos, desde la bioinformática hasta el análisis financiero, permitiendo el manejo eficaz de grandes volúmenes de datos.\n",
        "\n",
        "# Estas herramientas y estrategias te permitirán manejar datos de manera más eficiente, lo cual es fundamental en el mundo de la ciencia de datos. Practicar estos conceptos y técnicas puede convertirte en un experto en la optimización y escalabilidad con NumPy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8seBOdEopm3P",
        "outputId": "3db3a7ab-05fb-46b1-dccb-ea7e33f102b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados de la paralelización: [49938.391033099055, 50006.46431281321, 49990.44049430368, 49939.055408452325, 50018.124481618484, 49970.095366122216, 49994.6382751885, 50008.53651054885, 49934.198387664655, 49862.18887691926]\n"
          ]
        }
      ]
    }
  ]
}